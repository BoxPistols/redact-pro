# RedactPro

AI-powered PII masking tool for Japanese resumes and documents.

æ—¥æœ¬èªå±¥æ­´æ›¸ãƒ»è·å‹™çµŒæ­´æ›¸ã®å€‹äººæƒ…å ±ã‚’è‡ªå‹•æ¤œå‡ºã—ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚

## Features

- ğŸ¯ **16ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œ** â€” PDF, Word, Excel, ODS, CSV, Markdown, HTML, RTF, JSON, ODT, TXT
- ğŸ¤– **ãƒãƒ«ãƒAIãƒ—ãƒ­ãƒã‚¤ãƒ€** â€” Claude / OpenAI / Gemini
- ğŸ” **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œå‡º** â€” æ­£è¦è¡¨ç¾ + è¾æ›¸ + AI + ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹
- ğŸŒ **URLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°** â€” Wantedly, LinkedIn, LAPRASç­‰ã‹ã‚‰ã®å–ã‚Šè¾¼ã¿
- ğŸ“‹ **HTML/ãƒ†ã‚­ã‚¹ãƒˆè²¼ä»˜** â€” CORSå›é¿ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸŒ™ **ãƒ€ãƒ¼ã‚¯/ãƒ©ã‚¤ãƒˆãƒ†ãƒ¼ãƒ** â€” CSS Custom Properties
- ğŸ“± **ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œ** â€” ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³
- ğŸ“¤ **4å½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ** â€” TXT, Markdown, JSON, CSV

## Quick Start

```bash
# Clone
git clone https://github.com/<your-org>/redact-pro.git
cd redact-pro

# Install
pnpm install

# Environment
cp .env.example .env.local

# Dev server
pnpm dev
```

Open [http://localhost:3000](http://localhost:3000)

## Project Structure

```
redact-pro/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Root layout (metadata, lang)
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Entry point
â”‚   â”‚   â”œâ”€â”€ RedactPro.tsx       # Main app (monolith, migrating)
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ scrape/
â”‚   â”‚           â””â”€â”€ route.ts    # Server-side URL proxy (no CORS)
â”‚   â””â”€â”€ types/                  # (planned) TypeScript definitions
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ REFACTOR_PLAN.md        # Module decomposition roadmap
â”œâ”€â”€ public/
â”œâ”€â”€ .env.example
â”œâ”€â”€ next.config.ts
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

## Architecture

### Current: Monolith Phase

`RedactPro.tsx` is a single ~1500 line client component containing all UI and logic.  
This is intentional â€” the code was developed iteratively as a Claude Artifact and is being migrated to a proper Next.js project.

See [docs/REFACTOR_PLAN.md](docs/REFACTOR_PLAN.md) for the decomposition roadmap.

### URL Scraping

The key benefit of Next.js: **server-side scraping proxy**.

```
Browser â†’ /api/scrape?url=https://wantedly.com/... â†’ Server fetches â†’ Returns HTML
```

No CORS issues. No unreliable third-party proxies. The client automatically detects the local API route.

### Fallback Chain (client-side)

When `/api/scrape` is unavailable (e.g. static export):

1. Direct fetch (CORS-enabled sites only)
2. allOrigins proxy
3. everyOrigin proxy  
4. CodeTabs proxy
5. Corsfix proxy
6. Error â†’ "ãƒ†ã‚­ã‚¹ãƒˆ/HTMLè²¼ä»˜" tab guidance

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SCRAPE_ENABLED` | `true` | Enable/disable scraping API |
| `SCRAPE_RATE_LIMIT` | `30` | Max requests per minute per IP |
| `SCRAPE_ALLOWED_DOMAINS` | (all) | Comma-separated domain allowlist |

## Deploy

### Vercel (recommended)

```bash
pnpm i -g vercel
vercel
```

### Docker

```dockerfile
FROM node:20-slim AS base
RUN corepack enable pnpm

WORKDIR /app
COPY package.json pnpm-lock.yaml ./
RUN pnpm install --frozen-lockfile

COPY . .
RUN pnpm build

EXPOSE 3000
CMD ["pnpm", "start"]
```

## License

MIT
